import string
import csv
import os
import re
import datetime
from collections import defaultdict

PRINTABLE_BYTES = bytearray(string.printable,"UTF8")

''' 
                                    FrcLogToCSV
    
    Background and Overview:
    FRC logfiles, which have an extension of .dsevents, are created by the Driver Station
    and can be viewed there by clicking on the gear icon in the pane toward the right
    of the DS window. Then click on View Log File. This will open a new window called the
    "Driver Station Log File Viewer", which is a nice tool for looking at warnings, errors,
    and information messages ("Event List" tab) as well as viewing a graph of electrical
    usage, CAN %, etc. ("Data and Events vs Time" tab). However, sometimes it can be 
    difficult to view the logger messages generated by the MD code because the messages can be
    truncated and interspersed with many other FRC-generated messages. Unfortunately, the
    .dsevents file is binary and cannot be read easily outside of the DS Log File Viewer.
    Another issue with the dsevents file is that the events are not necessarily stored
    chronologically in the file; they are close but not exact so it is confusing when you
    read a sequence of log messages.

    To address these problems, you can run this script which reads one or more .dsevents file
    and generates a CSV file that contains just messages. The CSV file can be opened in
    a spreadsheet and easily browsed and filtered. The spreadsheet can also be sorted 
    so that the messages appear in chronological order.

    How to Use:
    1. Run this script
    2. Enter the directory where the .dsevents files reside. Normally this directory is 
       /Users/Public/Documents/FRC/Log Files. You can check in the Driver Station Log File Viewer
       to be sure.
    3. Enter the name of a single log file (including the .dsevents extension), or enter "all"
       which will process all .dsevents logfiles in the directory specified above
    4. Open the generated csv file in Excel, Sheets, or whatever. The csv filename is the
       same as the original .dsevents file plus a .csv extension. So, xxx.dsevents will a generate
       xxx.dsevents.csv file.

    Log File Contents:
    The .dsevents file is organized as follows:
        1. Header, consisting of a version number and timestamp
        2. Series of records, each record consists of a timestamp, count, and string (of length count)

    Messages in the .dsevents file show up in the following sequence of elements in a record:
        <TagVersion>, which always seems to be followed by '1 '
        <time>, followed by some numeric string which is a bit confusing
        <message>, followed by the message text
        [Note: there can be other elements besides <message> after a <time>]

    Mater Dei Messages:
    The Logger class has a number of methods for outputting messages to the logfile: setup(),
    info(), etc. These logger messages show up in the .dsevents file after the <message> element
    described above, but the message content is composed of the following fields:
        0x1b - A beginning byte with a value of 27
        Color information - A sequence of characters that define the color for the message. This
                            is mainly useful for displaying in VSCode. The color codes are
                            obscure strings like "[38;5;249m"
        Logger type - "setup", "info", etc.
        Time of day - hh:mm:ss.sssss
        Separator string - this is always " :: "
        Log message - Something like "Initializing Command: SwerveDrive... "
'''

MAX_INT64 = 2**63 - 1

class DSEventParser():
    def __init__(self, input_file):
        self.strm = open(input_file, 'rb')
        self.version = None
        self.start_time = None

        #TODO Read on demand rather than pre-loading entire contents of the file
        self.contents = self.strm.read()
        self.x = 0
        self.filelength = len(self.contents)

        self.read_header()
        return

    def close(self):
        self.strm.close()
        return

    def read_timestamp(self):
        ''' The following is the original code:
        # Time stamp: int64, uint64
        b1 = self.strm.read(8)
        b2 = self.strm.read(8)
        if not b1 or not b2:
            return None
        sec = struct.unpack('>q', b1)[0]
        millisec = struct.unpack('>Q', b2)[0]

        # for now, ignore
        dt = datetime.datetime(1904, 1, 1, 0, 0, 0, tzinfo=datetime.timezone.utc)
        dt += datetime.timedelta(seconds=(sec + float(millisec) / MAX_INT64))
        return dt
        '''

        #TODO Make sure date/time is correct
        # For now, just skip over the timestamp and give half-hearted attempt
        sec = self.unpack_number("seconds")
        if not sec:
            return None

        millisec = self.unpack_number("milliseconds")
        if not millisec:
            return None

        dt = datetime.datetime(1904, 1, 1, 0, 0, 0, tzinfo=datetime.timezone.utc)
        dt += datetime.timedelta(seconds=(sec + float(millisec) / MAX_INT64))
        return dt

    def unpack_bytes(self, nbytes):
        ''' Unpack bytes into a numeric value
        '''

        #TODO Handle signed & unsigned numbers

        # Check to see if we are at the end of the file
        if self.x >= self.filelength:
            return None

        value = 0

        for i in range(0,nbytes):
            byte = self.contents[self.x]
            byte_value = byte * 256**(nbytes-i-1)
            value += byte_value
            #print(f"{hex(self.x)}: {byte} = {byte)} - which contributes a value of {byte_value}")
            self.x += 1

        return value

    def unpack_number(self, type):
        ''' Unpack a string of bytes into a number
            Type is ["int", "seconds", "milliseconds"]
            Note: the original version used the struct libary but this is commented out
                  (for now) in order to avoid any PYPI dependencies
        '''

        if type == "int":
            # value = struct.unpack('>i', self.strm.read(4))[0]
            value = self.unpack_bytes(4)

        elif type == "seconds":
            # int64
            #b1 = self.strm.read(8)
            #if not b1:
            #    return None
            #sec = struct.unpack('>q', b1)[0]
            value = self.unpack_bytes(8)

        elif type == "milliseconds":
            # uint64
            #b2 = self.strm.read(8)
            #if not b2:
            #    return None
            #millisec = struct.unpack('>Q', b2)[0]
            value = self.unpack_bytes(8)

        else:
            raise Exception(f"Unknown unpacking type: {type}")

        return value

    def unpack_string(self, str_length):
        ''' Unpack a sequence of bytes into a string
            Note: the original version used the struct libary but this is commented out
                  (for now) in order to avoid any PYPI dependencies
        '''

        #msg = struct.unpack('%ds' % msg_len, self.strm.read(str_length))[0]
        #msg = str.decode('ascii', "backslashreplace")

        try:
            msg = str(self.contents[self.x: self.x + str_length], "UTF8")
        except:
            print(f"Skipping message {self.contents[self.x: self.x + str_length]}")
            msg = ""

        self.x += str_length

        return msg

    def read_records(self):
        if self.version not in [3, 4]:
            raise Exception("Unknown file version number {}".format(self.version))

        while True:
            r = self.read_record_v3_or_v4()
            if r is None:
                break
            yield r
        return

    def read_header(self):
        #self.version = struct.unpack('>i', self.strm.read(4))[0]
        self.version = self.unpack_number("int")
        if self.version not in [3, 4]:
            raise Exception("Unknown file version number {}".format(self.version))
        self.start_time = self.read_timestamp()  # file starttime
        return

    def read_record_v3_or_v4(self):
        t = self.read_timestamp()
        if t is None:
            return None

        #print(f"Reading record length at {hex(self.x)}")

        #msg_len = struct.unpack('>i', self.strm.read(4))[0]
        msg_len = self.unpack_number("int")

        #msg = struct.unpack('%ds' % msg_len, self.strm.read(msg_len))[0]
        #msg = msg.decode('ascii', "backslashreplace")
        msg = self.unpack_string(msg_len)

        return {'time': t, 'message': msg}

    @staticmethod
    def find_match_info(filename):
        rdr = DSEventParser(filename)
        try:
            for rec in rdr.read_records():
                m = re.match(r'FMS Connected:\s+(?P<match>.*),\s+Field Time:\s+(?P<time>[0-9/ :]*)', rec['message'])
                if m:
                    return {'match_name': m.group('match'),
                            'field_time': datetime.datetime.strptime(m.group('time'), '%y/%m/%d %H:%M:%S')}
        finally:
            rdr.close()
        return None

TAGVERSION_ELEMENT_STR = "<TagVersion>"
TAGVERSION_ELEMENT_LENGTH = len(TAGVERSION_ELEMENT_STR)

TIME_ELEMENT_STR = "<time>"
TIME_ELEMENT_LENGTH = len(TIME_ELEMENT_STR)

MESSAGE_ELEMENT_STR = "<message> "
MESSAGE_ELEMENT_LENGTH = len(MESSAGE_ELEMENT_STR)

# The following dictionary records the types of logging messages that the Mater Dei logger records
md_messages = {"setup"   : "[38;5;249msetup --> ",
                "waiting" : "[38;5;201mwaiting --> ",
                "action"  : "[32mACTION --> ",
                "info"    : "[0mINFO --> ",
                "ending"  : "[34mending --> ",
                "warning" : "[33mWARNING --> ",
                "problem" : "[31mPROBLEM --> ",
                "debug"   : "[0m" + "DEBUG --> ",
                }

class CSVWriter():
    ''' Class to write the csv file
    '''

    def __init__(self):
        self.csv_file = None
        self.n_md_messages_found = 0
        self.n_frc_messages_found = 0

    def process_message(self, message_str, element_time):
        ''' Process the content of the <message> element.
            If this is a message generated by the MD logger then pick it apart
        '''

        found_md_message = False
        if message_str:
            # MD messages start with a 0x1b
            if ord(message_str[0]) == 0x1b:
                md_message = message_str[1:]

                for md_message_type, md_message_header in md_messages.items():

                    if md_message.startswith(md_message_header):

                        # Extract time and logger message
                        non_header_start = len(md_message_header)
                        non_header = md_message[non_header_start:]
                        time, logger_msg = non_header.split(" :: ")
                        time = time.lstrip()

                        #print(f"Found MD Message: type: {md_message_type}, time={time}; logger msg={logger_msg}")
                        found_md_message = True
                        #n_md_messages_found += 1

                        line = ["MD Logger", time, md_message_type, logger_msg, element_time, ""]
                        self.csv_file.writerow(line)

                        break

            if not found_md_message:
                #n_frc_messages_found += 1

                line = ["FRC", "", "", "", element_time, message_str]
                self.csv_file.writerow(line)

                #print(f"FRC message at position {hex(x)}: {msg_content}")

        return found_md_message


    def process_tagversion_record(self, tagversion_record):
        ''' The tagversion_record can contain a series of elements. Each series starts
            with a <TagVersion> element and a <time> element. After that we are only
            interested in <message> elements
        '''

        #print(f"Parsing: {tagversion_record}")

        # Look for the first <TagVersion> element
        tagversionElement_start = tagversion_record.find(TAGVERSION_ELEMENT_STR)
        while tagversionElement_start >= 0:

            # Look for the <time> element
            tagversionContent_start = tagversionElement_start + TAGVERSION_ELEMENT_LENGTH
            timeElement_start = tagversion_record.find(TIME_ELEMENT_STR, tagversionContent_start)
            if timeElement_start <0:
                break

            # Tag version content is between the TagVerson and Time elements
            tagversion_content = tagversion_record[tagversionContent_start: timeElement_start]

            # Look for next element
            timeContent_start = timeElement_start + TIME_ELEMENT_LENGTH
            elementAfterTime_start = tagversion_record.find("<", timeContent_start)
            if elementAfterTime_start < 0:
                break

            # Time content is between the Time element and the next element (whatever that might be)
            time_content = tagversion_record[timeContent_start: elementAfterTime_start]

            # Find the rest of the tag
            elementAfterTime_end = tagversion_record.find(">", elementAfterTime_start)
            if elementAfterTime_end < 0:
                break

            # Get the name of the tag
            next_tag = tagversion_record[elementAfterTime_start+1 : elementAfterTime_end]

            if next_tag == "message":
                messageContent_start = elementAfterTime_end + 1
                elementAfterMessage_start = tagversion_record.find("<", messageContent_start)
                if elementAfterMessage_start > 0:
                    # There is another element so the message content goes up to it
                    message_content = tagversion_record[messageContent_start : elementAfterMessage_start]
                else:
                    # The message is the last element so use the remainder of the string
                    message_content = tagversion_record[messageContent_start :]
                # Remove leading and trailing whitespace
                message_content = message_content.strip()

                # Done with this sequence of elements so we can process it
                #print(f"Found message: {message_content}")

                is_md_message = self.process_message(message_content, time_content)
                if is_md_message:
                    self.n_md_messages_found += 1
                elif message_content:
                    self.n_frc_messages_found += 1
            else:
                # Not a message element
                pass

            # See if there is another TagVersion element; if so, do it all again
            tagversionElement_start = tagversion_record.find(TAGVERSION_ELEMENT_STR, elementAfterTime_start)


    def process_logfile(self, logfile_directory, logfile_file):
        ''' Parse .dsevents filename in the given directory and generate csv file
        '''

        self.n_frc_messages_found = 0
        self.n_md_messages_found = 0

        # Dictionary used to keep track of number of records of each type
        record_type_counter = defaultdict(int)

        known_record_types = {
            "Info Joystick" : "skip",
            "Info roboRIO" : "skip",
            "Info Java" : "process",
            "FMS Connected" : "process",
            "<TagVersion>" : "process",
            "Game Specific Data" : "process",
            "Info Rail Faults" : "process",
            "Code Start Notification" : "process",
            "Warning" : "process"
            }
    
        logfile_path = os.path.join(logfile_directory, logfile_file)
        csv_path = logfile_path + '.csv'

        # Create and open CSV file for output
        outfile = open(csv_path, 'w', newline='')
        self.csv_file = csv.writer(outfile)

        # Print header row to CSV file
        header = ["Event","MD Time","MD Logger Type","MD Logger Message", "FRC Time","FRC Message"]
        self.csv_file.writerow(header)

        dsEventParser = DSEventParser(logfile_path)

        print(f"\nReading log file: {logfile_path}")

        try:
            match_name = None
            for rec in dsEventParser.read_records():
                msg_time_stamp = rec["time"]
                message = rec["message"]
                #print(f"Record: {message[0:25]}")

                known_record_type = False
                for record_type, action in known_record_types.items():
                    if message.startswith(record_type):
                        if action != "skip":
                            if record_type == "<TagVersion>":
                                self.process_tagversion_record(message)
                        known_record_type = True
                        record_type_counter[record_type] += 1
                        break

                if not known_record_type:
                    record_type_counter["<Unknown>"] += 1
                    print(f"Unknown record:{message}")

                m = re.match(r'FMS Connected:\s+(?P<match>.*),\s+Field Time:\s+(?P<time>[0-9/ :]*)', message)
                if m:
                    match_name = m.group('match')
                    field_time = datetime.datetime.strptime(m.group('time'), '%y/%m/%d %H:%M:%S')

            # Print summary
            if match_name:
                print(f"Match Name = {match_name}; Field Time = {field_time}")
            else:
                print("Not a match")
            print("\nSummary of # record types found:")
            for record_type, count in record_type_counter.items():
                print(f" {record_type}: {count}")
            print(f"\nMessages found in the log file:")
            print(f"  {self.n_frc_messages_found:>6} Generated by FRC code")
            print(f"  {self.n_md_messages_found:>6} Generated by Mater Dei logger")
            print(f"\nWrote CSV file: {csv_path}")
            print('-----------------------------------------------------------------')

        except:
            print(f"Unable to parse file {logfile_path}")

        dsEventParser.close()

def generate_csv(options):
    ''' Driver to read one or more .dsevents logfile and generate one or more csv files
    '''

    logfile_directory = options["logfile_directory"]
    which_files = options["which_files"]

    csv_writer = CSVWriter()

    if which_files == "single":
        filename = options["logfile_name"]
        csv_writer.process_logfile(logfile_directory, filename)

    elif which_files in ["all", "competition_only"]:
        for filename in os.listdir(logfile_directory):
            if filename.endswith(".dsevents"):

                #print(f"Processing logfile {file}")

                if which_files == "competition_only":
                    match = DSEventParser.find_match_info(logfile_path)
                    if not match:
                        continue

                csv_writer.process_logfile(logfile_directory, logfile_path)

    else:
        raise Exception(f"Unknown 'which_files' option: {which_files}")

def main():
    ''' Get options and start the ball rolling
    '''

    options = {"logfile_directory" : "C:\\Temp\\logfiles",
               "which_files" : "single",                            # "single", "all", "competition_only"
               "logfile_name" : "2023_03_11 17_30_45 Sat.dsevents",
               "combine_csv" : False,
              }

    # TODO Get options from command line rather than prompting
    command_line_args = True
    if command_line_args:
        options["which_files"] = "competition_only"
        generate_csv(options)

    else:
        logfile_directory = input("Enter logfile directory (or q to quit):")
        if logfile_directory != 'q':
            options["logfile_directory"] = logfile_directory
            while True:
                logfile_name = input("Enter name of .dsevents file ('q' to quit, 'all' for all dsevents files): ")
                if logfile_name == 'q':
                    break
                if logfile_name == 'all':
                    options["which_files"] = "all"

                else:
                    options["which_files"] = "single"
                    options["logfile_name"] = logfile_name

                print("\n")
                generate_csv(options)

main()