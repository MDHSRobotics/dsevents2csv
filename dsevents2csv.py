import string
import csv
import os
import re
import datetime
from collections import defaultdict

PRINTABLE_BYTES = bytearray(string.printable,"UTF8")

''' 
                                    FrcLogToCSV
    
    Background and Overview:
    FRC logfiles, which have an extension of .dsevents, are created by the Driver Station
    and can be viewed there by clicking on the gear icon in the pane toward the right
    of the DS window. Then click on View Log File. This will open a new window called the
    "Driver Station Log File Viewer", which is a nice tool for looking at warnings, errors,
    and information messages ("Event List" tab) as well as viewing a graph of electrical
    usage, CAN %, etc. ("Data and Events vs Time" tab). However, sometimes it can be 
    difficult to view the logger messages generated by the MD code because the messages can be
    truncated and interspersed with many other FRC-generated messages. Unfortunately, the
    .dsevents file is binary and cannot be read easily outside of the DS Log File Viewer.
    Another issue with the dsevents file is that the events are not necessarily stored
    chronologically in the file; they are close but not exact so it is confusing when you
    read a sequence of log messages.

    To address these problems, you can run this script which reads one or more .dsevents file
    and generates a CSV file that contains just messages. The CSV file can be opened in
    a spreadsheet and easily browsed and filtered. The spreadsheet can also be sorted 
    so that the messages appear in chronological order.

    How to Use:
    1. Run this script
    2. Enter the directory where the .dsevents files reside. Normally this directory is 
       /Users/Public/Documents/FRC/Log Files. You can check in the Driver Station Log File Viewer
       to be sure.
    3. Enter the name of a single log file (including the .dsevents extension), or enter "all"
       which will process all .dsevents logfiles in the directory specified above
    4. Open the generated csv file in Excel, Sheets, or whatever. The csv filename is the
       same as the original .dsevents file plus a .csv extension. So, xxx.dsevents will a generate
       xxx.dsevents.csv file.

    Log File Contents:
    Messages in the .dsevents file seem to show up in the following sequence of elements:
        <TagVersion>, which always seems to be followed by '1 '
        <time>, followed by some numeric string which is a bit confusing
        <message>, followed by the message text

    Mater Dei Messages:
    The Logger class has a number of methods for outputting messages to the logfile: setup(),
    info(), etc. These logger messages show up in the .dsevents file after the <message> element
    described above, but the message content is composed of the following fields:
        0x1b - A beginning byte with a value of 27
        Color information - A sequence of characters that define the color for the message. This
                            is mainly useful for displaying in VSCode. The color codes are
                            obscure strings like "[38;5;249m"
        Logger type - "setup", "info", etc.
        Time of day - hh:mm:ss.sssss
        Separator string - this is always " :: "
        Log message - Something like "Initializing Command: SwerveDrive... "
'''

MAX_INT64 = 2**63 - 1

class DSEventParser():
    def __init__(self, input_file):
        self.strm = open(input_file, 'rb')
        self.version = None
        self.start_time = None

        #TODO Read on demand rather than pre-loading entire contents of the file
        self.contents = self.strm.read()
        self.x = 0
        self.filelength = len(self.contents)

        self.read_header()
        return

    def close(self):
        self.strm.close()
        return

    def read_timestamp(self):
        ''' The following is the original code:
        # Time stamp: int64, uint64
        b1 = self.strm.read(8)
        b2 = self.strm.read(8)
        if not b1 or not b2:
            return None
        sec = struct.unpack('>q', b1)[0]
        millisec = struct.unpack('>Q', b2)[0]

        # for now, ignore
        dt = datetime.datetime(1904, 1, 1, 0, 0, 0, tzinfo=datetime.timezone.utc)
        dt += datetime.timedelta(seconds=(sec + float(millisec) / MAX_INT64))
        return dt
        '''

        #TODO Make sure date/time is correct
        # For now, just skip over the timestamp and give half-hearted attempt
        sec = self.unpack_number("seconds")
        if not sec:
            return None

        millisec = self.unpack_number("milliseconds")
        if not millisec:
            return None

        dt = datetime.datetime(1904, 1, 1, 0, 0, 0, tzinfo=datetime.timezone.utc)
        dt += datetime.timedelta(seconds=(sec + float(millisec) / MAX_INT64))
        return dt

    def unpack_bytes(self, nbytes):
        ''' Unpack bytes into a numeric value
        '''

        #TODO Handle signed & unsigned numbers

        # Check to see if we are at the end of the file
        if self.x >= self.filelength:
            return None

        value = 0

        for i in range(0,nbytes):
            byte = self.contents[self.x]
            byte_value = byte * 256**(nbytes-i-1)
            value += byte_value
            #print(f"{hex(self.x)}: {byte} = {byte)} - which contributes a value of {byte_value}")
            self.x += 1

        return value

    def unpack_number(self, type):
        ''' Unpack a string of bytes into a number
            Type is ["int", "seconds", "milliseconds"]
            Note: the original version used the struct libary but this is commented out
                  (for now) in order to avoid any PYPI dependencies
        '''

        if type == "int":
            # value = struct.unpack('>i', self.strm.read(4))[0]
            value = self.unpack_bytes(4)

        elif type == "seconds":
            # int64
            #b1 = self.strm.read(8)
            #if not b1:
            #    return None
            #sec = struct.unpack('>q', b1)[0]
            value = self.unpack_bytes(8)

        elif type == "milliseconds":
            # uint64
            #b2 = self.strm.read(8)
            #if not b2:
            #    return None
            #millisec = struct.unpack('>Q', b2)[0]
            value = self.unpack_bytes(8)

        else:
            raise Exception(f"Unknown unpacking type: {type}")

        return value

    def unpack_string(self, str_length):
        ''' Unpack a sequence of bytes into a string
            Note: the original version used the struct libary but this is commented out
                  (for now) in order to avoid any PYPI dependencies
        '''

        #msg = struct.unpack('%ds' % msg_len, self.strm.read(str_length))[0]
        #msg = str.decode('ascii', "backslashreplace")

        msg = str(self.contents[self.x: self.x + str_length], "UTF8")

        self.x += str_length

        return msg

    def read_records(self):
        if self.version not in [3, 4]:
            raise Exception("Unknown file version number {}".format(self.version))

        while True:
            r = self.read_record_v3_or_v4()
            if r is None:
                break
            yield r
        return

    def read_header(self):
        #self.version = struct.unpack('>i', self.strm.read(4))[0]
        self.version = self.unpack_number("int")
        if self.version not in [3, 4]:
            raise Exception("Unknown file version number {}".format(self.version))
        self.start_time = self.read_timestamp()  # file starttime
        return

    def read_record_v3_or_v4(self):
        t = self.read_timestamp()
        if t is None:
            return None

        #print(f"Reading record length at {hex(self.x)}")

        #msg_len = struct.unpack('>i', self.strm.read(4))[0]
        msg_len = self.unpack_number("int")

        #msg = struct.unpack('%ds' % msg_len, self.strm.read(msg_len))[0]
        #msg = msg.decode('ascii', "backslashreplace")
        msg = self.unpack_string(msg_len)

        return {'time': t, 'message': msg}

    @staticmethod
    def find_match_info(filename):
        rdr = DSEventParser(filename)
        try:
            for rec in rdr.read_records():
                m = re.match(r'FMS Connected:\s+(?P<match>.*),\s+Field Time:\s+(?P<time>[0-9/ :]*)', rec['message'])
                if m:
                    return {'match_name': m.group('match'),
                            'field_time': datetime.datetime.strptime(m.group('time'), '%y/%m/%d %H:%M:%S')}
        finally:
            rdr.close()
        return None

FMSCONNECTED_ELEMENT_STR = "FMS Connected:"
FMSCONNECTED_ELEMENT_BYTE_SEQUENCE = FMSCONNECTED_ELEMENT_STR.encode("utf8")
FMSCONNECTED_ELEMENT_LEN = len(FMSCONNECTED_ELEMENT_BYTE_SEQUENCE)

TAGVERSION_ELEMENT_STR = "<TagVersion>"
TAGVERSION_ELEMENT_BYTE_SEQUENCE = TAGVERSION_ELEMENT_STR.encode("utf8")
TAGVERSION_ELEMENT_LENGTH = len(TAGVERSION_ELEMENT_BYTE_SEQUENCE)

TIME_ELEMENT_STR = "<time>"
TIME_ELEMENT_BYTE_SEQUENCE = TIME_ELEMENT_STR.encode("utf8")
TIME_ELEMENT_LENGTH = len(TIME_ELEMENT_BYTE_SEQUENCE)

MESSAGE_ELEMENT_STR = "<message> "
MESSAGE_ELEMENT_BYTE_SEQUENCE = MESSAGE_ELEMENT_STR.encode("utf8")
MESSAGE_ELEMENT_LENGTH = len(MESSAGE_ELEMENT_BYTE_SEQUENCE)

# The following dictionary records the types of logging messages that the Mater Dei logger records
md_messages = {"setup"   : "[38;5;249msetup --> ".encode("utf8"),
               "waiting" : "[38;5;201mwaiting --> ".encode("utf8"),
               "action"  : "[32mACTION --> ".encode("utf8"),
               "info"    : "[0mINFO --> ".encode("utf8"),
               "ending"  : "[34mending --> ".encode("utf8"),
               "warning" : "[33mWARNING --> ".encode("utf8"),
               "problem" : "[31mPROBLEM --> ".encode("utf8"),
              }

def process_logfile_messages(logfile_directory, logfile_name):
    ''' Read messages in a dsevents log file from the given directory and create a csv file
    '''

    logfile_path = os.path.join(logfile_directory, logfile_name)
    csv_path = logfile_path + '.csv'

    # Create and open CSV file for output
    with open(csv_path, 'w', newline='') as outfile:
        csv_file = csv.writer(outfile)

        # Print header row to CSV file
        header = ["Event","MD Time","MD Logger Type","MD Logger Message","LOC", "FRC Time","FRC Message"]
        csv_file.writerow(header)

        # Open the dsevent file in binary mode
        with open(logfile_path, mode="rb") as log_file:
            # Read the entire logfile into an array
            # Important note: this is an array of bytes (which is immutable), not a bytearray (which is)
            contents = log_file.read()

        log_file_length = len(contents)

        x = 0
        while x < log_file_length:
            # Look for FMS Connected
            next_slice = contents[x:x+FMSCONNECTED_ELEMENT_LEN]
            if next_slice == FMSCONNECTED_ELEMENT_BYTE_SEQUENCE:
                byte1 = contents[x-2]
                byte2 = contents[x-1]
                record_length = byte1 * 16 + byte2
                fms_string = str(contents[x:x+record_length-1],"UTF8")

                print(f"Found FMS>>{fms_string} at position {hex(x)}")
                print(f"Preceding two bytes: {byte1}, {byte2}; value = {record_length}")

                m = re.match(r'FMS Connected:\s+(?P<match>.*),\s+Field Time:\s+(?P<time>[0-9/ :]*)', fms_string)
                if m:
                    match_name = m.group('match')
                    field_time = datetime.datetime.strptime(m.group('time'), '%y/%m/%d %H:%M:%S')
                    print(f"Match Name = {match_name}; Field Time = {field_time}")

                print(f"Length of fms string is {len(fms_string)}")

                x += record_length
                print(f"\nBefore next record:")
                for y in range(x-2, x):
                    print(f"{hex(y)}: {contents[y]} = {hex(contents[y])} = {chr(contents[y])}")
                
                print("\nNext Record:")
                for y in range(x, x+20):
                    print(f"{hex(y)}: {contents[y]} = {hex(contents[y])} = {chr(contents[y])}")
                break
            x += 1

        n_messages_found = 0
        n_md_messages_found = 0
        n_frc_messages_found = 0

        # Step through the file byte by byte; there might be more efficent ways to do this but it's not bad.
        # The variable x is the current byte location being interpreted
        x = 0
        while x < log_file_length:

            # Look for the next <TagVersion> element
            next_slice = contents[x:x+TAGVERSION_ELEMENT_LENGTH]
            if next_slice == TAGVERSION_ELEMENT_BYTE_SEQUENCE:

                x += TAGVERSION_ELEMENT_LENGTH

                # Skip over 2-digit tag version number
                version_number = contents[x:x+2]
                if version_number != "1 ".encode("utf8"):
                    raise Exception(f"Expected '1 ' at position {hex(x)}")

                x += 2

                # Look for <time> element
                next_slice = contents[x: x+TIME_ELEMENT_LENGTH]
                if next_slice != TIME_ELEMENT_BYTE_SEQUENCE:
                    raise Exception(f"Expected <time> element at position {hex(x)}")

                x += TIME_ELEMENT_LENGTH

                # Get content after the time element, up to next element
                time_element_content = ''
                while contents[x] != ord('<'):
                    time_element_content += chr(contents[x])
                    x += 1
                #print(f"Time: {time_element_content}")

                next_slice = contents[x:x+MESSAGE_ELEMENT_LENGTH]
                #print(f"{next_slice}")

                # The next element might be a message, but it could be something else
                # (like <count> which we don't know/care about)
                if next_slice != MESSAGE_ELEMENT_BYTE_SEQUENCE:
                    #print(f"Unrecognized element after <time> element at position {hex(x)}: {next_slice}")
                    pass
                else:
                    #print(f"I found <message> at position {hex(x)}")
                    n_messages_found += 1
                    message_location = x

                    # Skip over message element
                    x += MESSAGE_ELEMENT_LENGTH

                    found_md_message = False
                    # MD messages start with a 0x1b
                    if contents[x] == 0x1b:

                        # y is a temporary pointer; we don't want to advance x until we know sure this is a MD message
                        y = 1

                        # Extract all of the content after the 0x1b up to the next element or the next nonprintable char
                        md_msg_content = bytearray()
                        while contents[x+y] != ord('<') and contents[x+y] in PRINTABLE_BYTES:
                            md_msg_content.append(contents[x+y])
                            y += 1

                        #print(f"Testing at position {hex(x)}: md_msg_content of {md_msg_content}")
                        for md_message_type, md_message_header in md_messages.items():
                            md_message_header_len = len(md_message_header)

                            #print(f"comparing {md_msg_content[0:md_message_header_len]} and {md_message_header}")
                            if md_msg_content[0:md_message_header_len] == md_message_header:

                                # Extract time and logger message
                                non_header = str(md_msg_content[md_message_header_len:], "UTF8")
                                time, logger_msg = non_header.split(" :: ")
                                time = time.lstrip()

                                #print(f"Found MD Message at position {hex(x)}: type: {md_message_type}, time={time}; logger msg={logger_msg}")
                                n_md_messages_found += 1
                                found_md_message = True

                                line = ["MD Logger", time, md_message_type, logger_msg, hex(message_location), time_element_content, ""]
                                csv_file.writerow(line)

                                # We found a valid MD message so advance the pointer beyond it
                                x += y
                                break

                    if not found_md_message:

                        # Get message content
                        msg_content = bytearray()
                        # Extract all of the content after the message tag up to the next element or the next nonprintable char
                        while contents[x] != ord('<') and contents[x] in PRINTABLE_BYTES:
                            msg_content.append(contents[x])
                            x += 1

                        n_frc_messages_found += 1

                        line = ["FRC", "", "", "", hex(message_location), time_element_content, str(msg_content, "UTF8")]
                        csv_file.writerow(line)

                        #print(f"FRC message at position {hex(x)}: {msg_content}")

            else:
                x += 1

    # Print summary
    print(f"\nRead log file: {logfile_path} ({log_file_length:,} bytes)")
    print(f"Messages found in the log file:")
    print(f"  {n_frc_messages_found:>6} Generated by FRC code")
    print(f"  {n_md_messages_found:>6} Generated by Mater Dei logger")
    print(f"  ------")
    print(f"  {n_messages_found:>6} Total")
    print(f"Wrote CSV file: {csv_path}")
    print('-----------------------------------------------------------------')

def process_logfile_records(logfile_directory, logfile_name):
    ''' Read records in a dsevents log file from the given directory and create a csv file
    '''

    # Dictionary used to keep track of number of records of each type
    record_type_counter = defaultdict(int)
    
    logfile_path = os.path.join(logfile_directory, logfile_name)

    # Open the dsevent file in binary mode
    with open(logfile_path, mode="rb") as log_file:
        # Read the entire logfile into an array
        # Important note: this is an array of bytes (which is immutable), not a bytearray (which is)
        contents = log_file.read()

    log_file_length = len(contents)

    x = 0

    VERSION_NUMBER_LENGTH = 4
    TIMESTAMP_LENGTH = 16

    # Skip over the header
    x += VERSION_NUMBER_LENGTH
    x += TIMESTAMP_LENGTH 

    while x < log_file_length:
        # Read next record

        # Skip over timestamp
        x += TIMESTAMP_LENGTH

        # Get record length
        record_length = 0
        #print(f"Reading record length at {hex(x)}")
        for i in range(0,4):
            byte = contents[x]
            value = byte * 256**(3-i)
            record_length += value
            #print(f"{hex(x)}: {contents[x]} = {hex(contents[x])} - which contributes a value of {value}")
            x += 1

        #print(f"Computed record length of {record_length}")

        printable_len = min(record_length, 512)
        record_str = str(contents[x: x + printable_len], "UTF8")

        known_record_types = {
            "Info Joystick" : "skip",
            "Info roboRIO" : "skip",
            "FMS Connected" : "process",
            "<TagVersion>" : "process",
            }
          
        known_record_type = False
        for record_type, action in known_record_types.items():
            if record_str.startswith(record_type):
                if action != "skip":
                    print(f"Record string:{record_str}")
                known_record_type = True
                record_type_counter[record_type] += 1
                break

        if not known_record_type:
            record_type_counter["<Unknown>"] += 1

        m = re.match(r'FMS Connected:\s+(?P<match>.*),\s+Field Time:\s+(?P<time>[0-9/ :]*)', record_str)
        if m:
            match_name = m.group('match')
            field_time = datetime.datetime.strptime(m.group('time'), '%y/%m/%d %H:%M:%S')
            print(f"Match Name = {match_name}; Field Time = {field_time}")

        x += record_length

    print("Summary of # record types found:")
    for record_type, count in record_type_counter.items():
        print(f" {record_type}: {count}")

def process_logfile(logfile_directory, logfile_name):
    ''' Parse .dsevents filename in the given directory and generate csv file
    '''

    # Dictionary used to keep track of number of records of each type
    record_type_counter = defaultdict(int)

    known_record_types = {
        "Info Joystick" : "skip",
        "Info roboRIO" : "skip",
        "FMS Connected" : "process",
        "<TagVersion>" : "process",
        "Game Specific Data" : "process",
        "Info Rail Faults" : "process",
        "Code Start Notification" : "process",
        "Warning" : "process"
        }
    
    logfile_path = os.path.join(logfile_directory, logfile_name)
    dsEventParser = DSEventParser(logfile_path)

    for rec in dsEventParser.read_records():
        msg_time_stamp = rec["time"]
        message = rec["message"]
        #print(f"Record: {message[0:25]}")

        known_record_type = False
        for record_type, action in known_record_types.items():
            if message.startswith(record_type):
                if action != "skip":
                    pass
                known_record_type = True
                record_type_counter[record_type] += 1
                break

        if not known_record_type:
            record_type_counter["<Unknown>"] += 1
            print(f"Unknown record:{message}")

        m = re.match(r'FMS Connected:\s+(?P<match>.*),\s+Field Time:\s+(?P<time>[0-9/ :]*)', message)
        if m:
            match_name = m.group('match')
            field_time = datetime.datetime.strptime(m.group('time'), '%y/%m/%d %H:%M:%S')
            print(f"Match Name = {match_name}; Field Time = {field_time}")

    print("Summary of # record types found:")
    for record_type, count in record_type_counter.items():
        print(f" {record_type}: {count}")

    dsEventParser.close()

def generate_csv(options):
    ''' Driver to read one or more .dsevents logfile and generate one or more csv files
    '''

    logfile_directory = options["logfile_directory"]
    which_files = options["which_files"]

    if which_files == "single":
        filename = options["logfile_name"]
        process_logfile(logfile_directory, filename)

    elif which_files == "all":
        for filename in os.listdir(logfile_directory):
            if filename.endswith(".dsevents"):
                #print(f"Processing logfile {file}")
                process_logfile(logfile_directory, filename)

    else:
        raise Exception(f"Unknown 'which_files' option: {which_files}")

def main():
    ''' Get options and start the ball rolling
    '''

    options = {"logfile_directory" : "",
               "which_files" : "",          # "single", "all", "competition_only"
               "logfile_name" : "",
               "combine_csv" : False,
              }

    # TODO Get options from command line rather than prompting
    command_line_args = True
    if command_line_args:
        process_logfile("C:\\Temp", "2023_03_11 17_30_45 Sat.dsevents")

    else:
        logfile_directory = input("Enter logfile directory (or q to quit):")
        if logfile_directory != 'q':
            options["logfile_directory"] = logfile_directory
            while True:
                logfile_name = input("Enter name of .dsevents file ('q' to quit, 'all' for all dsevents files): ")
                if logfile_name == 'q':
                    break
                if logfile_name == 'all':
                    options["which_files"] = "all"

                else:
                    options["which_files"] = "single"
                    options["logfile_name"] = logfile_name
                    process_logfile_records(logfile_directory, logfile_name)

main()